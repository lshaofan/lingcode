#!/usr/bin/env python3
"""
FunASR è½¬å½•è„šæœ¬
æ”¯æŒå¤šç§æ¨¡å‹çš„ç¦»çº¿è¯­éŸ³è¯†åˆ«ï¼ˆä½¿ç”¨ä¸­å›½é•œåƒæºï¼‰
"""

import sys
import json
import os
import argparse
from pathlib import Path
from typing import Optional, Dict, Any


def transcribe_audio(
    audio_path: str,
    model_name: str = "paraformer-zh",
    language: Optional[str] = None,
    hotword: Optional[str] = None,
) -> Dict[str, Any]:
    """
    ä½¿ç”¨ FunASR è½¬å½•éŸ³é¢‘

    Args:
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        model_name: æ¨¡å‹åç§°
        language: è¯­è¨€ä»£ç ï¼ˆå¯é€‰ï¼‰
        hotword: çƒ­è¯ï¼ˆå¯é€‰ï¼‰

    Returns:
        è½¬å½•ç»“æœå­—å…¸
    """
    # è®¾ç½®ç¼“å­˜ç›®å½•ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿ FunASR/ModelScope ä½¿ç”¨æ­£ç¡®çš„ç¼“å­˜è·¯å¾„ï¼‰
    cache_root = os.path.expanduser(os.environ.get("MODELSCOPE_CACHE", "~/.cache/modelscope"))
    os.environ["MODELSCOPE_CACHE"] = cache_root

    # è°ƒè¯•ï¼šæ‰“å° Python ç¯å¢ƒä¿¡æ¯
    print(f"ğŸ Python executable: {sys.executable}", file=sys.stderr)
    print(f"ğŸ Python version: {sys.version}", file=sys.stderr)
    print(f"ğŸ Python path: {sys.path[:3]}", file=sys.stderr)
    print(f"ğŸ“ MODELSCOPE_CACHE: {cache_root}", file=sys.stderr)

    try:
        from funasr import AutoModel
        print(f"âœ… FunASR imported successfully", file=sys.stderr)
    except ImportError as e:
        print(f"âŒ Failed to import FunASR: {e}", file=sys.stderr)
        return {
            "success": False,
            "error": f"FunASR not installed or import failed: {str(e)}. Please install with: pip install funasr",
        }

    try:
        # æ¨¡å‹é…ç½®æ˜ å°„ï¼ˆä½¿ç”¨å®Œæ•´çš„ ModelScope IDï¼Œä¸ download å‡½æ•°ä¸€è‡´ï¼‰
        model_configs = {
            "paraformer-zh": {
                "model": "damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
                "vad_model": "damo/speech_fsmn_vad_zh-cn-16k-common-pytorch",
                "punc_model": "damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch",
            },
            "paraformer-large": {
                "model": "iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
                "vad_model": "damo/speech_fsmn_vad_zh-cn-16k-common-pytorch",
                "punc_model": "damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch",
            },
            "sensevoice-small": {
                "model": "iic/SenseVoiceSmall",
                "vad_model": "damo/speech_fsmn_vad_zh-cn-16k-common-pytorch",
                "punc_model": None,  # SenseVoice è‡ªå¸¦æ ‡ç‚¹
            },
        }

        config = model_configs.get(model_name)
        if not config:
            return {
                "success": False,
                "error": f"Unknown model: {model_name}",
            }

        # è·å–ç¼“å­˜ç›®å½•ï¼ˆä¸ download å‡½æ•°ä¿æŒä¸€è‡´ï¼‰
        cache_root = os.path.expanduser(os.environ.get("MODELSCOPE_CACHE", "~/.cache/modelscope"))

        # åˆå§‹åŒ–æ¨¡å‹
        model_kwargs = {
            "model": config["model"],
            "disable_log": False,
            "disable_pbar": False,
            "disable_update": True,  # ğŸš€ å…³é”®ä¼˜åŒ–ï¼šç¦ç”¨è‡ªåŠ¨æ›´æ–°æ£€æŸ¥ï¼Œé¿å…é‡å¤ä¸‹è½½
            "hub": "ms",  # ä½¿ç”¨ ModelScope hub
            "model_revision": None,  # ä¸æŒ‡å®šç‰ˆæœ¬ï¼Œä½¿ç”¨æœ€æ–°ç‰ˆæœ¬
            "cache_dir": cache_root,  # ğŸ”‘ å…³é”®ï¼šæŒ‡å®šç¼“å­˜ç›®å½•ï¼Œé¿å…é‡å¤ä¸‹è½½
        }

        if config["vad_model"]:
            model_kwargs["vad_model"] = config["vad_model"]

        if config["punc_model"]:
            model_kwargs["punc_model"] = config["punc_model"]

        print(f"ğŸ”§ Model cache_dir: {cache_root}", file=sys.stderr)
        print(f"ğŸ”§ Loading model: {config['model']}", file=sys.stderr)

        model = AutoModel(**model_kwargs)

        # å‡†å¤‡è¾“å…¥å‚æ•°
        generate_kwargs = {"input": audio_path}

        if hotword:
            generate_kwargs["hotword"] = hotword

        if language:
            generate_kwargs["language"] = language

        # æ‰§è¡Œè½¬å½•
        result = model.generate(**generate_kwargs)

        if not result or len(result) == 0:
            return {
                "success": False,
                "error": "No transcription result",
            }

        # æå–æ–‡æœ¬
        text = result[0].get("text", "")

        return {
            "success": True,
            "text": text,
            "raw_result": result[0],
        }

    except Exception as e:
        return {
            "success": False,
            "error": f"Transcription failed: {str(e)}",
        }


def download_model(
    model_name: str,
    cache_dir: Optional[str] = None,
    max_retries: int = 3,
) -> Dict[str, Any]:
    """
    ä¸‹è½½ FunASR æ¨¡å‹ï¼ˆä½¿ç”¨ä¸­å›½é•œåƒæºï¼Œè·³è¿‡å·²ä¸‹è½½çš„æ¨¡å‹ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œé‡è¯•ï¼‰

    Args:
        model_name: æ¨¡å‹åç§°
        cache_dir: ç¼“å­˜ç›®å½•
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤3æ¬¡ï¼‰

    Returns:
        ä¸‹è½½ç»“æœ
    """
    try:
        from modelscope.hub.snapshot_download import snapshot_download
        from modelscope.hub.file_download import http_get_file
    except ImportError:
        print(f"âŒ ModelScope æœªå®‰è£…ï¼Œæ— æ³•ä¸‹è½½æ¨¡å‹", file=sys.stderr)
        print(f"ğŸ’¡ æç¤ºï¼šModelScope æ­£åœ¨åå°å®‰è£…ä¸­ï¼Œè¯·ç¨å€™ç‰‡åˆ»åé‡è¯•", file=sys.stderr)
        return {
            "success": False,
            "error": "ModelScope æ­£åœ¨å®‰è£…ä¸­ï¼Œè¯·ç¨å€™ 10-30 ç§’åé‡è¯•",
        }

    import time

    try:
        # è®¾ç½®ä¸­å›½é•œåƒæºï¼ˆå¦‚æœæœªè®¾ç½®ï¼‰
        if not os.environ.get("MODELSCOPE_ENDPOINT"):
            os.environ["MODELSCOPE_ENDPOINT"] = "https://www.modelscope.cn"

        # è·å–ç¼“å­˜æ ¹ç›®å½•
        cache_root = os.path.expanduser(os.environ.get("MODELSCOPE_CACHE", "~/.cache/modelscope"))
        if cache_dir:
            cache_root = cache_dir
            os.environ["MODELSCOPE_CACHE"] = cache_dir

        # ModelScope çš„å®é™…ç¼“å­˜ç»“æ„
        hub_models_dir = os.path.join(cache_root, "hub", "models")

        model_ids = {
            "paraformer-zh": "damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
            "paraformer-large": "iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
            "sensevoice-small": "iic/SenseVoiceSmall",
            "fsmn-vad": "damo/speech_fsmn_vad_zh-cn-16k-common-pytorch",
            "ct-punc": "damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch",
        }

        model_id = model_ids.get(model_name)
        if not model_id:
            return {
                "success": False,
                "error": f"Unknown model: {model_name}",
            }

        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨çš„è¾…åŠ©å‡½æ•°
        def is_model_downloaded(model_id_to_check):
            org, model_name_part = model_id_to_check.split("/", 1)
            model_cache_path = os.path.join(hub_models_dir, org, model_name_part)
            if not (os.path.exists(model_cache_path) and os.path.isdir(model_cache_path)):
                return False
            # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…çš„æ¨¡å‹æ–‡ä»¶
            for root, dirs, files in os.walk(model_cache_path):
                if files:
                    return True
            return False

        # ä¸‹è½½å•ä¸ªæ¨¡å‹çš„è¾…åŠ©å‡½æ•°ï¼ˆå¸¦é‡è¯•ï¼‰
        def download_with_retry(model_id_to_download, display_name, retries=max_retries):
            for attempt in range(1, retries + 1):
                try:
                    print(f"ğŸ“¥ [{attempt}/{retries}] æ­£åœ¨ä¸‹è½½{display_name}: {model_id_to_download}", file=sys.stderr)
                    print(f"PROGRESS:0:{display_name}:å¼€å§‹ä¸‹è½½", file=sys.stderr)  # è¿›åº¦æ ‡è®°
                    sys.stderr.flush()

                    # ModelScope çš„ snapshot_download æ”¯æŒæ–­ç‚¹ç»­ä¼ 
                    # æ³¨æ„ï¼šprogress_callback åŠŸèƒ½åœ¨æŸäº›ç‰ˆæœ¬ä¸­ä¸ç¨³å®šï¼Œæˆ‘ä»¬ä½¿ç”¨ç®€å•çš„å¼€å§‹/ç»“æŸæ ‡è®°
                    result_dir = snapshot_download(
                        model_id_to_download,
                        cache_dir=cache_root,
                    )

                    print(f"PROGRESS:100:{display_name}:ä¸‹è½½å®Œæˆ", file=sys.stderr)  # è¿›åº¦æ ‡è®°
                    sys.stderr.flush()
                    print(f"âœ… {display_name}ä¸‹è½½å®Œæˆ: {model_id_to_download}", file=sys.stderr)
                    return result_dir

                except Exception as e:
                    error_msg = str(e)
                    print(f"âŒ [{attempt}/{retries}] {display_name}ä¸‹è½½å¤±è´¥: {error_msg}", file=sys.stderr)
                    sys.stderr.flush()

                    if attempt < retries:
                        wait_time = attempt * 2  # é€’å¢ç­‰å¾…æ—¶é—´ï¼š2s, 4s, 6s
                        print(f"â³ {wait_time}ç§’åé‡è¯•...", file=sys.stderr)
                        sys.stderr.flush()
                        time.sleep(wait_time)
                    else:
                        raise Exception(f"{display_name}ä¸‹è½½å¤±è´¥ï¼ˆå·²é‡è¯•{retries}æ¬¡ï¼‰: {error_msg}")

        # ä¸‹è½½ä¸»æ¨¡å‹ï¼ˆå¦‚æœæœªä¸‹è½½ï¼‰
        if is_model_downloaded(model_id):
            print(f"âœ… ä¸»æ¨¡å‹å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {model_id}", file=sys.stderr)
            print(f"PROGRESS:100:ä¸»æ¨¡å‹:å·²å­˜åœ¨", file=sys.stderr)  # è¿›åº¦æ ‡è®°
            org, model_name_part = model_id.split("/", 1)
            model_dir = os.path.join(hub_models_dir, org, model_name_part)
        else:
            model_dir = download_with_retry(model_id, "ä¸»æ¨¡å‹")

        # ä¸‹è½½ä¾èµ–æ¨¡å‹ï¼ˆVAD å’Œ PUNCï¼‰
        dependencies = []
        if model_name in ["paraformer-zh", "paraformer-large"]:
            dependencies = ["fsmn-vad", "ct-punc"]
        elif model_name == "sensevoice-small":
            dependencies = ["fsmn-vad"]

        for dep in dependencies:
            dep_id = model_ids.get(dep)
            if dep_id:
                try:
                    if is_model_downloaded(dep_id):
                        print(f"âœ… ä¾èµ–æ¨¡å‹å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {dep}", file=sys.stderr)
                        print(f"PROGRESS:100:{dep}:å·²å­˜åœ¨", file=sys.stderr)  # è¿›åº¦æ ‡è®°
                    else:
                        download_with_retry(dep_id, f"ä¾èµ–æ¨¡å‹({dep})")
                except Exception as e:
                    print(f"âš ï¸  ä¸‹è½½ä¾èµ–æ¨¡å‹ {dep} å¤±è´¥: {e}", file=sys.stderr)
                    print(f"PROGRESS:0:{dep}:ä¸‹è½½å¤±è´¥", file=sys.stderr)  # è¿›åº¦æ ‡è®°
                    # ä¸è¦å› ä¸ºä¾èµ–æ¨¡å‹å¤±è´¥è€Œæ•´ä½“å¤±è´¥ï¼ˆæŸäº›æ¨¡å‹å¯èƒ½å¯é€‰ï¼‰
                    pass

        print(f"ğŸ‰ æ¨¡å‹ {model_name} åŠæ‰€æœ‰ä¾èµ–å‡†å¤‡å®Œæˆ", file=sys.stderr)

        return {
            "success": True,
            "model_dir": str(model_dir),
            "model_size": get_dir_size(str(model_dir)),
        }

    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        print(f"âŒ ä¸‹è½½å¤±è´¥è¯¦æƒ…:\n{error_details}", file=sys.stderr)
        return {
            "success": False,
            "error": f"Model download failed: {str(e)}",
        }


def get_dir_size(path: str) -> str:
    """è·å–ç›®å½•å¤§å°ï¼ˆç”¨äºæ˜¾ç¤ºä¸‹è½½è¿›åº¦ï¼‰"""
    try:
        total_size = 0
        for dirpath, dirnames, filenames in os.walk(path):
            for filename in filenames:
                filepath = os.path.join(dirpath, filename)
                if os.path.exists(filepath):
                    total_size += os.path.getsize(filepath)

        # è½¬æ¢ä¸º MB
        size_mb = total_size / (1024 * 1024)
        return f"{size_mb:.1f} MB"
    except:
        return "Unknown"


def check_model_exists(
    model_name: str,
    cache_dir: Optional[str] = None,
) -> Dict[str, Any]:
    """
    æ£€æŸ¥ FunASR æ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼ˆåŒ…æ‹¬ä¾èµ–æ¨¡å‹ï¼‰

    Args:
        model_name: æ¨¡å‹åç§°
        cache_dir: ç¼“å­˜ç›®å½•

    Returns:
        æ£€æŸ¥ç»“æœ
    """
    try:
        import modelscope
    except ImportError:
        # ModelScope æœªå®‰è£…æ—¶ï¼Œæ¨¡å‹è‚¯å®šä¸å­˜åœ¨ï¼Œè¿”å›æˆåŠŸä½† exists=False
        print(f"âš ï¸  ModelScope æœªå®‰è£…ï¼Œæ— æ³•æ£€æŸ¥æ¨¡å‹", file=sys.stderr)
        return {
            "success": True,
            "error": "",
            "exists": False,
        }

    try:
        # è·å–ç¼“å­˜æ ¹ç›®å½• - ModelScope é»˜è®¤ä½¿ç”¨ ~/.cache/modelscope
        cache_root = os.path.expanduser(os.environ.get("MODELSCOPE_CACHE", "~/.cache/modelscope"))
        if cache_dir:
            cache_root = cache_dir

        # å½“ä½¿ç”¨ snapshot_download æ—¶ï¼Œå¦‚æœæŒ‡å®šäº† cache_dirï¼Œæ¨¡å‹ç›´æ¥ä¸‹è½½åˆ° {cache_dir}/{org}/{model_name}
        # å¦‚æœæ²¡æœ‰æŒ‡å®š cache_dirï¼ŒModelScope é»˜è®¤ä½¿ç”¨ {cache_root}/hub/models/{org}/{model_name}
        # ä½†åœ¨æˆ‘ä»¬çš„ä¸‹è½½å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬æŒ‡å®šäº† cache_dir=cache_rootï¼Œæ‰€ä»¥è·¯å¾„æ˜¯ {cache_root}/{org}/{model_name}
        hub_models_dir = cache_root

        # æ¨¡å‹ ID æ˜ å°„
        model_ids = {
            "paraformer-zh": "damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
            "paraformer-large": "iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
            "sensevoice-small": "iic/SenseVoiceSmall",
            "fsmn-vad": "damo/speech_fsmn_vad_zh-cn-16k-common-pytorch",
            "ct-punc": "damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch",
        }

        model_id = model_ids.get(model_name)
        if not model_id:
            return {
                "success": False,
                "error": f"Unknown model: {model_name}",
                "exists": False,
            }

        # æ£€æŸ¥ä¸»æ¨¡å‹
        def check_single_model(model_id_to_check, model_display_name=""):
            # ModelScope æœ‰ä¸¤ç§å¯èƒ½çš„è·¯å¾„ç»“æ„:
            # 1. æ–°ç‰ˆæœ¬ (æŒ‡å®š cache_dir): {cache_dir}/{org}/{model_name}
            # 2. æ—§ç‰ˆæœ¬ (é»˜è®¤è·¯å¾„): {cache_dir}/hub/models/{org}/{model_name}
            # æˆ‘ä»¬éœ€è¦åŒæ—¶æ£€æŸ¥è¿™ä¸¤ç§è·¯å¾„
            org, model_name_part = model_id_to_check.split("/", 1)

            # è·¯å¾„1: æ–°ç‰ˆæœ¬è·¯å¾„ (ç›´æ¥åœ¨ cache_root ä¸‹)
            path1 = os.path.join(hub_models_dir, org, model_name_part)
            # è·¯å¾„2: æ—§ç‰ˆæœ¬è·¯å¾„ (åœ¨ hub/models ä¸‹)
            path2 = os.path.join(hub_models_dir, "hub", "models", org, model_name_part)

            print(f"ğŸ” æ£€æŸ¥æ¨¡å‹ {model_display_name}:", file=sys.stderr)
            print(f"   è·¯å¾„1 (æ–°): {path1}", file=sys.stderr)
            print(f"   è·¯å¾„2 (æ—§): {path2}", file=sys.stderr)

            # å°è¯•æ£€æŸ¥ä¸¤ä¸ªè·¯å¾„
            for path_idx, model_cache_path in enumerate([path1, path2], 1):
                if not os.path.exists(model_cache_path):
                    print(f"  è·¯å¾„{path_idx}: âŒ ä¸å­˜åœ¨", file=sys.stderr)
                    continue

                if not os.path.isdir(model_cache_path):
                    print(f"  è·¯å¾„{path_idx}: âŒ ä¸æ˜¯ç›®å½•", file=sys.stderr)
                    continue

                # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…çš„æ¨¡å‹æ–‡ä»¶
                has_files = False
                file_count = 0
                for root, dirs, files in os.walk(model_cache_path):
                    if files:
                        has_files = True
                        file_count = len(files)
                        break

                if has_files:
                    print(f"  è·¯å¾„{path_idx}: âœ… æ‰¾åˆ° {file_count} ä¸ªæ–‡ä»¶", file=sys.stderr)
                    return True
                else:
                    print(f"  è·¯å¾„{path_idx}: âŒ ç›®å½•ä¸ºç©º", file=sys.stderr)

            print(f"  âŒ æ‰€æœ‰è·¯å¾„éƒ½æœªæ‰¾åˆ°æœ‰æ•ˆæ¨¡å‹", file=sys.stderr)
            return False

        # æ£€æŸ¥ä¸»æ¨¡å‹
        print(f"ğŸ“‹ å¼€å§‹æ£€æŸ¥æ¨¡å‹: {model_name}", file=sys.stderr)
        print(f"ğŸ“ ç¼“å­˜æ ¹ç›®å½•: {cache_root}", file=sys.stderr)
        print(f"ğŸ“ æ¨¡å‹ç›®å½•: {hub_models_dir}", file=sys.stderr)

        if not check_single_model(model_id, f"{model_name} (ä¸»æ¨¡å‹)"):
            print(f"âŒ ä¸»æ¨¡å‹æœªæ‰¾åˆ°: {model_id}", file=sys.stderr)
            return {
                "success": True,
                "exists": False,
            }

        # æ£€æŸ¥ä¾èµ–æ¨¡å‹
        dependencies = []
        if model_name in ["paraformer-zh", "paraformer-large"]:
            dependencies = ["fsmn-vad", "ct-punc"]
        elif model_name == "sensevoice-small":
            dependencies = ["fsmn-vad"]

        for dep in dependencies:
            dep_id = model_ids.get(dep)
            if dep_id and not check_single_model(dep_id, f"{dep} (ä¾èµ–)"):
                print(f"âŒ ä¾èµ–æ¨¡å‹æœªæ‰¾åˆ°: {dep_id}", file=sys.stderr)
                return {
                    "success": True,
                    "exists": False,
                }

        # æ‰€æœ‰æ¨¡å‹éƒ½å­˜åœ¨
        org, model_name_part = model_id.split("/", 1)
        model_cache_path = os.path.join(hub_models_dir, org, model_name_part)
        print(f"âœ… æ¨¡å‹åŠæ‰€æœ‰ä¾èµ–å·²ä¸‹è½½: {model_name}", file=sys.stderr)
        return {
            "success": True,
            "exists": True,
            "model_path": model_cache_path,
        }

    except Exception as e:
        return {
            "success": False,
            "error": f"Check failed: {str(e)}",
            "exists": False,
        }


def main():
    parser = argparse.ArgumentParser(description="FunASR è½¬å½•å·¥å…·")
    parser.add_argument("command", choices=["transcribe", "download", "check"], help="å‘½ä»¤")
    parser.add_argument("--audio", help="éŸ³é¢‘æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model", default="paraformer-zh", help="æ¨¡å‹åç§°")
    parser.add_argument("--language", help="è¯­è¨€ä»£ç ")
    parser.add_argument("--hotword", help="çƒ­è¯")
    parser.add_argument("--cache-dir", help="æ¨¡å‹ç¼“å­˜ç›®å½•")

    args = parser.parse_args()

    if args.command == "transcribe":
        if not args.audio:
            result = {
                "success": False,
                "error": "--audio is required for transcribe command",
            }
        else:
            result = transcribe_audio(
                audio_path=args.audio,
                model_name=args.model,
                language=args.language,
                hotword=args.hotword,
            )
    elif args.command == "download":
        result = download_model(
            model_name=args.model,
            cache_dir=args.cache_dir,
        )
    elif args.command == "check":
        result = check_model_exists(
            model_name=args.model,
            cache_dir=args.cache_dir,
        )
    else:
        result = {
            "success": False,
            "error": f"Unknown command: {args.command}",
        }

    # è¾“å‡º JSON ç»“æœ
    print(json.dumps(result, ensure_ascii=False))


if __name__ == "__main__":
    main()
