use std::path::PathBuf;
use tauri::{AppHandle, Emitter, Manager, State};

/// æ¨¡å‹å¼•æ“ç±»å‹
#[derive(serde::Serialize, serde::Deserialize, Clone, Debug, PartialEq)]
#[serde(rename_all = "lowercase")]
pub enum ModelEngine {
    Whisper,
    FunASR,
}

/// æ¨¡å‹ä¿¡æ¯
#[derive(serde::Serialize, serde::Deserialize, Clone)]
pub struct ModelInfo {
    pub name: String,
    pub engine: ModelEngine,
    pub size: String,
    pub size_bytes: u64,
    pub speed: String,
    pub accuracy: String,
    pub is_recommended: bool,
    pub is_downloaded: bool,
    pub download_url: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub description: Option<String>,
}

/// è·å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹ï¼ˆWhisper + FunASRï¼‰
#[tauri::command]
pub fn get_available_models(app: AppHandle) -> Result<Vec<ModelInfo>, String> {
    let models_dir = get_models_dir(&app)?;

    let mut models = vec![];

    // Whisper æ¨¡å‹
    models.extend(vec![
        ModelInfo {
            name: "base".to_string(),
            engine: ModelEngine::Whisper,
            size: "74MB".to_string(),
            size_bytes: 74 * 1024 * 1024,
            speed: "å¿«é€Ÿ".to_string(),
            accuracy: "ä¸€èˆ¬ç²¾åº¦".to_string(),
            is_recommended: false,
            is_downloaded: check_model_downloaded(&models_dir, "base"),
            download_url: "https://hf-mirror.com/ggerganov/whisper.cpp/resolve/main/ggml-base.bin".to_string(),
            description: Some("Whisper åŸºç¡€æ¨¡å‹ï¼Œæ”¯æŒå¤šè¯­è¨€".to_string()),
        },
        ModelInfo {
            name: "small".to_string(),
            engine: ModelEngine::Whisper,
            size: "244MB".to_string(),
            size_bytes: 244 * 1024 * 1024,
            speed: "è¾ƒå¿«".to_string(),
            accuracy: "è¾ƒé«˜ç²¾åº¦".to_string(),
            is_recommended: false,
            is_downloaded: check_model_downloaded(&models_dir, "small"),
            download_url: "https://hf-mirror.com/ggerganov/whisper.cpp/resolve/main/ggml-small.bin".to_string(),
            description: Some("Whisper å°å‹æ¨¡å‹ï¼Œå¹³è¡¡é€Ÿåº¦å’Œç²¾åº¦".to_string()),
        },
        ModelInfo {
            name: "medium".to_string(),
            engine: ModelEngine::Whisper,
            size: "769MB".to_string(),
            size_bytes: 769 * 1024 * 1024,
            speed: "è¾ƒæ…¢".to_string(),
            accuracy: "é«˜ç²¾åº¦".to_string(),
            is_recommended: false,
            is_downloaded: check_model_downloaded(&models_dir, "medium"),
            download_url: "https://hf-mirror.com/ggerganov/whisper.cpp/resolve/main/ggml-medium.bin".to_string(),
            description: Some("Whisper ä¸­å‹æ¨¡å‹ï¼Œé«˜ç²¾åº¦".to_string()),
        },
        ModelInfo {
            name: "large".to_string(),
            engine: ModelEngine::Whisper,
            size: "1.5GB".to_string(),
            size_bytes: 1536 * 1024 * 1024,
            speed: "æ…¢".to_string(),
            accuracy: "æœ€é«˜ç²¾åº¦".to_string(),
            is_recommended: false,
            is_downloaded: check_model_downloaded(&models_dir, "large"),
            download_url: "https://hf-mirror.com/ggerganov/whisper.cpp/resolve/main/ggml-large.bin".to_string(),
            description: Some("Whisper å¤§å‹æ¨¡å‹ï¼Œæœ€é«˜ç²¾åº¦".to_string()),
        },
    ]);

    // FunASR æ¨¡å‹
    models.extend(vec![
        ModelInfo {
            name: "paraformer-zh".to_string(),
            engine: ModelEngine::FunASR,
            size: "~220MB".to_string(),
            size_bytes: 220 * 1024 * 1024,
            speed: "å¿«é€Ÿ".to_string(),
            accuracy: "é«˜ç²¾åº¦ï¼ˆä¸­æ–‡ï¼‰".to_string(),
            is_recommended: true,
            is_downloaded: check_funasr_model_downloaded(&app, "paraformer-zh"),
            download_url: "modelscope://damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch".to_string(),
            description: Some("é˜¿é‡Œ FunASR ä¸­æ–‡è¯†åˆ«æ¨¡å‹ï¼Œä¸“ä¸ºä¸­æ–‡ä¼˜åŒ–".to_string()),
        },
        ModelInfo {
            name: "paraformer-large".to_string(),
            engine: ModelEngine::FunASR,
            size: "~380MB".to_string(),
            size_bytes: 380 * 1024 * 1024,
            speed: "è¾ƒå¿«".to_string(),
            accuracy: "æé«˜ç²¾åº¦ï¼ˆä¸­æ–‡ï¼‰".to_string(),
            is_recommended: false,
            is_downloaded: check_funasr_model_downloaded(&app, "paraformer-large"),
            download_url: "modelscope://iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch".to_string(),
            description: Some("FunASR å¤§å‹ä¸­æ–‡æ¨¡å‹ï¼Œæ›´é«˜ç²¾åº¦".to_string()),
        },
        ModelInfo {
            name: "sensevoice-small".to_string(),
            engine: ModelEngine::FunASR,
            size: "~160MB".to_string(),
            size_bytes: 160 * 1024 * 1024,
            speed: "å¿«é€Ÿ".to_string(),
            accuracy: "é«˜ç²¾åº¦ï¼ˆå¤šè¯­è¨€+æƒ…æ„Ÿï¼‰".to_string(),
            is_recommended: false,
            is_downloaded: check_funasr_model_downloaded(&app, "sensevoice-small"),
            download_url: "modelscope://iic/SenseVoiceSmall".to_string(),
            description: Some("æ”¯æŒå¤šè¯­è¨€å’Œæƒ…æ„Ÿè¯†åˆ«".to_string()),
        },
    ]);

    Ok(models)
}

/// ä¸‹è½½æ¨¡å‹
#[tauri::command]
pub async fn download_model(app: AppHandle, model_name: String) -> Result<(), String> {
    use tracing::info;

    info!("ğŸ“¥ [Model] Downloading model: {}", model_name);

    // å…ˆè·å–æ¨¡å‹åˆ—è¡¨ï¼Œç¡®å®šæ¨¡å‹ç±»å‹
    let models = get_available_models(app.clone())?;
    let model_info = models
        .iter()
        .find(|m| m.name == model_name)
        .ok_or_else(|| format!("Invalid model name: {}", model_name))?;

    info!("ğŸ“¥ [Model] Model engine: {:?}", model_info.engine);

    // æ ¹æ®æ¨¡å‹å¼•æ“ç±»å‹è°ƒç”¨ä¸åŒçš„ä¸‹è½½é€»è¾‘
    match model_info.engine {
        ModelEngine::FunASR => {
            // è°ƒç”¨ FunASR ä¸‹è½½å‡½æ•°
            info!("ğŸ“¥ [Model] Calling FunASR download function");

            // ç¡®ä¿ Python ç¯å¢ƒå¯ç”¨
            let python_env = crate::python::ensure_python_env(&app).await?;

            // ä¸‹è½½ FunASR æ¨¡å‹
            crate::funasr::download_funasr_model(&app, &python_env.python_path, &model_name).await?;

            Ok(())
        }
        ModelEngine::Whisper => {
            // Whisper æ¨¡å‹ä¸‹è½½é€»è¾‘ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
            info!("ğŸ“¥ [Model] Downloading Whisper model");
            let models_dir = get_models_dir(&app)?;
            std::fs::create_dir_all(&models_dir)
                .map_err(|e| format!("Failed to create models directory: {}", e))?;

            let model_path = models_dir.join(format!("ggml-{}.bin", model_name));

            // å¦‚æœæ¨¡å‹å·²å­˜åœ¨,å…ˆåˆ é™¤
            if model_path.exists() {
                std::fs::remove_file(&model_path)
                    .map_err(|e| format!("Failed to remove existing model: {}", e))?;
            }

            // è·å–ä¸‹è½½ URLï¼ˆä½¿ç”¨ä¸­å›½é•œåƒç«™ï¼‰
            let download_url = match model_name.as_str() {
                "base" => "https://hf-mirror.com/ggerganov/whisper.cpp/resolve/main/ggml-base.bin",
                "small" => "https://hf-mirror.com/ggerganov/whisper.cpp/resolve/main/ggml-small.bin",
                "medium" => "https://hf-mirror.com/ggerganov/whisper.cpp/resolve/main/ggml-medium.bin",
                "large" => "https://hf-mirror.com/ggerganov/whisper.cpp/resolve/main/ggml-large.bin",
                _ => return Err("Invalid Whisper model name".to_string()),
            };

            download_whisper_model(&app, &model_name, download_url, &model_path).await
        }
    }
}

/// Whisper æ¨¡å‹ä¸‹è½½é€»è¾‘ï¼ˆä»åŸ download_model å‡½æ•°ä¸­æå–ï¼‰
async fn download_whisper_model(
    app: &AppHandle,
    model_name: &str,
    download_url: &str,
    model_path: &PathBuf,
) -> Result<(), String> {
    // ä½¿ç”¨ reqwest ä¸‹è½½æ–‡ä»¶
    let client = reqwest::Client::new();
    let response = client
        .get(download_url)
        .send()
        .await
        .map_err(|e| format!("Failed to start download: {}", e))?;

    if !response.status().is_success() {
        return Err(format!("Download failed with status: {}", response.status()));
    }

    let total_size = response.content_length().unwrap_or(0);
    let mut downloaded: u64 = 0;

    // ä½¿ç”¨ tokio çš„æ–‡ä»¶å†™å…¥
    let mut file = tokio::fs::File::create(&model_path)
        .await
        .map_err(|e| format!("Failed to create file: {}", e))?;

    let mut stream = response.bytes_stream();
    use futures_util::StreamExt;
    use tokio::io::AsyncWriteExt;

    while let Some(chunk) = stream.next().await {
        let chunk = chunk.map_err(|e| format!("Failed to read chunk: {}", e))?;
        file.write_all(&chunk)
            .await
            .map_err(|e| format!("Failed to write chunk: {}", e))?;

        downloaded += chunk.len() as u64;

        // å‘é€è¿›åº¦äº‹ä»¶
        let progress = if total_size > 0 {
            (downloaded as f64 / total_size as f64 * 100.0) as u32
        } else {
            0
        };

        let _ = app.emit(
            "model-download-progress",
            DownloadProgress {
                model_name: model_name.to_string(),
                progress,
                downloaded,
                total: total_size,
            },
        );
    }

    file.flush()
        .await
        .map_err(|e| format!("Failed to flush file: {}", e))?;

    Ok(())
}

/// åˆ é™¤æ¨¡å‹
#[tauri::command]
pub fn delete_model(app: AppHandle, model_name: String) -> Result<(), String> {
    let models_dir = get_models_dir(&app)?;
    let model_path = models_dir.join(format!("ggml-{}.bin", model_name));

    if !model_path.exists() {
        return Err("Model not found".to_string());
    }

    std::fs::remove_file(&model_path).map_err(|e| format!("Failed to delete model: {}", e))?;

    Ok(())
}

/// è·å–å·²ä¸‹è½½çš„æ¨¡å‹åˆ—è¡¨
#[tauri::command]
pub fn get_downloaded_models(app: AppHandle) -> Result<Vec<ModelInfo>, String> {
    let models = get_available_models(app)?;
    Ok(models.into_iter().filter(|m| m.is_downloaded).collect())
}

/// è·å–æ¨¡å‹ç›®å½•è·¯å¾„ï¼ˆè°ƒè¯•ç”¨ï¼‰
#[tauri::command]
pub fn get_models_directory(app: AppHandle) -> Result<String, String> {
    let models_dir = get_models_dir(&app)?;
    Ok(models_dir.to_string_lossy().to_string())
}

// è¾…åŠ©å‡½æ•°

fn get_models_dir(app: &AppHandle) -> Result<PathBuf, String> {
    let app_data_dir = app
        .path()
        .app_data_dir()
        .map_err(|e| format!("Failed to get app data dir: {}", e))?;

    Ok(app_data_dir.join("models"))
}

fn check_model_downloaded(models_dir: &PathBuf, model_name: &str) -> bool {
    let model_path = models_dir.join(format!("ggml-{}.bin", model_name));
    model_path.exists()
}

/// æ£€æŸ¥ FunASR æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½
fn check_funasr_model_downloaded(app: &AppHandle, model_name: &str) -> bool {
    use std::process::Command;
    use tracing::info;

    // å°è¯•è·å– Python ç¯å¢ƒï¼ˆä½¿ç”¨åŒæ­¥æ£€æµ‹ï¼‰
    let python_env = match crate::python::detect_python(app) {
        Ok(env) => env,
        Err(e) => {
            info!("Failed to detect Python env for checking FunASR model: {}", e);
            return false;
        }
    };

    // è°ƒç”¨ Python è„šæœ¬æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    let script_path = match std::env::current_dir() {
        Ok(dir) => {
            // å¼€å‘æ¨¡å¼ï¼šä½¿ç”¨ src-tauri/scripts ç›®å½•
            #[cfg(debug_assertions)]
            {
                // å°è¯•ä¸¤ç§è·¯å¾„ï¼šå½“å‰ç›®å½•/scripts æˆ– å½“å‰ç›®å½•/src-tauri/scripts
                let script_path = if dir.ends_with("src-tauri") {
                    dir.join("scripts").join("funasr_transcribe.py")
                } else {
                    dir.join("src-tauri").join("scripts").join("funasr_transcribe.py")
                };

                if !script_path.exists() {
                    info!("Script not found: {:?}", script_path);
                    return false;
                }
                script_path
            }
            #[cfg(not(debug_assertions))]
            {
                // ç”Ÿäº§æ¨¡å¼ï¼šæš‚æœªå®ç°
                info!("Production mode not yet implemented");
                return false;
            }
        }
        Err(e) => {
            info!("Failed to get current dir: {}", e);
            return false;
        }
    };

    info!("ğŸ” Checking FunASR model '{}' with Python: {:?}", model_name, python_env.python_path);

    let output = Command::new(&python_env.python_path)
        .arg(&script_path)
        .arg("check")
        .arg("--model")
        .arg(model_name)
        .output();

    match output {
        Ok(output) => {
            if output.status.success() {
                let stdout = String::from_utf8_lossy(&output.stdout);
                info!("ğŸ Python script output: {}", stdout);

                // æå– JSON ç»“æœï¼ˆæœ€åä¸€è¡Œä»¥ { å¼€å¤´çš„å†…å®¹ï¼‰
                let json_line = stdout
                    .lines()
                    .filter(|line| line.trim().starts_with('{'))
                    .last();

                match json_line {
                    Some(line) => {
                        info!("ğŸ Extracted JSON line: {}", line);
                        match serde_json::from_str::<serde_json::Value>(line) {
                            Ok(json) => {
                                let exists = json.get("exists").and_then(|v| v.as_bool()).unwrap_or(false);
                                info!("âœ… FunASR model '{}' check result: exists = {}", model_name, exists);
                                exists
                            }
                            Err(e) => {
                                info!("âŒ Failed to parse check result for '{}': {}", model_name, e);
                                false
                            }
                        }
                    }
                    None => {
                        info!("âŒ No JSON result found in Python output for '{}'", model_name);
                        false
                    }
                }
            } else {
                let stderr = String::from_utf8_lossy(&output.stderr);
                info!("âŒ Failed to check FunASR model '{}': {}", model_name, stderr);
                false
            }
        }
        Err(e) => {
            info!("âŒ Command failed when checking FunASR model '{}': {}", model_name, e);
            false
        }
    }
}

#[derive(serde::Serialize, Clone)]
struct DownloadProgress {
    model_name: String,
    progress: u32,
    downloaded: u64,
    total: u64,
}

/// ä¸€é”®å®‰è£…FunASRå®Œæ•´ç¯å¢ƒï¼ˆPython + ä¾èµ– + æ¨¡å‹ï¼‰
/// è¿™æ˜¯ç”¨æˆ·é¦–æ¬¡ä½¿ç”¨FunASRæ—¶çš„ä¾¿æ·å‘½ä»¤
#[tauri::command]
pub async fn setup_funasr_environment(
    app: AppHandle,
    model_name: Option<String>,
    funasr_state: State<'_, crate::commands::funasr::FunASRState>,
) -> Result<(), String> {
    use crate::python::{ensure_python_env, install_funasr_with_progress};
    use tracing::info;

    let model = model_name.unwrap_or_else(|| "paraformer-zh".to_string());

    info!("ğŸš€ Starting FunASR environment setup with model: {}", model);

    // å‘é€å¼€å§‹äº‹ä»¶
    let _ = app.emit(
        "funasr-setup-status",
        SetupStatus {
            step: "å¼€å§‹".to_string(),
            progress: 0,
            message: "å¼€å§‹å®‰è£…FunASRç¯å¢ƒ...".to_string(),
            is_error: false,
        },
    );

    // æ­¥éª¤1: ç¡®ä¿Pythonç¯å¢ƒï¼ˆä¼šè‡ªåŠ¨å®‰è£…ä¾èµ–å¹¶æ˜¾ç¤ºè¿›åº¦ï¼‰
    info!("ğŸ“¦ Step 1/2: Setting up Python environment and dependencies...");
    let _ = app.emit(
        "funasr-setup-status",
        SetupStatus {
            step: "Pythonç¯å¢ƒ".to_string(),
            progress: 10,
            message: "æ­£åœ¨è®¾ç½®Pythonç¯å¢ƒå’Œä¾èµ–...".to_string(),
            is_error: false,
        },
    );

    match ensure_python_env(&app).await {
        Ok(python_env) => {
            info!("âœ… Python environment ready: {}", python_env.version);

            let _ = app.emit(
                "funasr-setup-status",
                SetupStatus {
                    step: "Pythonç¯å¢ƒ".to_string(),
                    progress: 50,
                    message: "Pythonç¯å¢ƒå·²å°±ç»ª".to_string(),
                    is_error: false,
                },
            );
        }
        Err(e) => {
            let error_msg = format!("Pythonç¯å¢ƒè®¾ç½®å¤±è´¥: {}", e);
            info!("âŒ {}", error_msg);

            let _ = app.emit(
                "funasr-setup-status",
                SetupStatus {
                    step: "Pythonç¯å¢ƒ".to_string(),
                    progress: 0,
                    message: error_msg.clone(),
                    is_error: true,
                },
            );

            return Err(error_msg);
        }
    }

    // æ­¥éª¤2: ä¸‹è½½FunASRæ¨¡å‹
    info!("ğŸ“¥ Step 2/2: Downloading FunASR model '{}'...", model);
    let _ = app.emit(
        "funasr-setup-status",
        SetupStatus {
            step: "ä¸‹è½½æ¨¡å‹".to_string(),
            progress: 60,
            message: format!("æ­£åœ¨ä¸‹è½½ {} æ¨¡å‹...", model),
            is_error: false,
        },
    );

    // ä½¿ç”¨ç°æœ‰çš„download_funasr_modelå‘½ä»¤
    match crate::commands::funasr::download_funasr_model(app.clone(), model.clone(), funasr_state).await {
        Ok(_) => {
            info!("âœ… Model '{}' downloaded successfully", model);

            let _ = app.emit(
                "funasr-setup-status",
                SetupStatus {
                    step: "å®Œæˆ".to_string(),
                    progress: 100,
                    message: "FunASRç¯å¢ƒå®‰è£…å®Œæˆï¼".to_string(),
                    is_error: false,
                },
            );

            Ok(())
        }
        Err(e) => {
            let error_msg = format!("æ¨¡å‹ä¸‹è½½å¤±è´¥: {}", e);
            info!("âŒ {}", error_msg);

            let _ = app.emit(
                "funasr-setup-status",
                SetupStatus {
                    step: "ä¸‹è½½æ¨¡å‹".to_string(),
                    progress: 60,
                    message: error_msg.clone(),
                    is_error: true,
                },
            );

            Err(error_msg)
        }
    }
}

/// ç¯å¢ƒè®¾ç½®çŠ¶æ€äº‹ä»¶
#[derive(serde::Serialize, Clone)]
struct SetupStatus {
    step: String,
    progress: u32,
    message: String,
    is_error: bool,
}